{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27fe6d96-9fbb-483d-af13-d97a30e95c7f",
   "metadata": {},
   "source": [
    "# Test Everything, Carefully: Generating Questions from Text\n",
    "## GA DSI 26 Capstone Project\n",
    "## Chapter 2: T5 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1c776d-bbfc-4661-bcd9-ed5160f27f15",
   "metadata": {},
   "source": [
    "In this notebook, we will be using pytorch and the hugging face libraries to fine-tune a T5 model and implementing it for our task at hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564e108c-27c7-4593-bd2b-12604b2b6cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 25429\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# modeling\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast as T5Tokenizer,\n",
    "    )\n",
    "from transformers.optimization import Adafactor\n",
    "\n",
    "# aesthetics\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*Trying to infer the `batch_size` from an ambiguous collection.*\"\n",
    ")\n",
    "seed_everything(25429)\n",
    "\n",
    "# scoring\n",
    "import spacy\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "357077e2-7796-46f1-b9a6-33ac094db2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30e8c8-1ec7-49d1-82ff-99965fe75189",
   "metadata": {},
   "source": [
    "# Unpickle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f71323-56bb-4489-9ba8-5171348a8423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      title  \\\n",
       "0  University_of_Notre_Dame   \n",
       "1  University_of_Notre_Dame   \n",
       "2  University_of_Notre_Dame   \n",
       "3  University_of_Notre_Dame   \n",
       "4  University_of_Notre_Dame   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                    answer  \n",
       "0               Saint Bernadette Soubirous  \n",
       "1                a copper statue of Christ  \n",
       "2                        the Main Building  \n",
       "3  a Marian place of prayer and reflection  \n",
       "4       a golden statue of the Virgin Mary  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_pickle('pickles/train.pkl')\n",
    "val = pd.read_pickle('pickles/test.pkl')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768676af-3dbb-494e-aa4a-bb8a09fe05e9",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e322f0b-b4d2-4c9e-8381-a82afda6a4fc",
   "metadata": {},
   "source": [
    "The T5TokenizerFast splits the text into unigrams, and adds additional end-of-sentence token, out-of-vocab token, and padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2805fb8-0a3c-4624-8745-38a31b51f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hug = 't5-small'\n",
    "t5tokenizer = T5Tokenizer.from_pretrained(hug)\n",
    "t5model = T5ForConditionalGeneration.from_pretrained(hug, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264597ab-200b-4c16-b959-7435865b32bc",
   "metadata": {},
   "source": [
    "We can define a separation token to split the context and answer. We will also add a random mask for the answers for the answer-agnostic question generation model. The masking chance will be a hyperparameter that balances between the accuracy of the masked answers with the unmasked ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9809cc-1144-4f51-ac6f-6833c8c0a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP_TOKEN = '<sep>'\n",
    "MASK_TOKEN = '[MASK]'\n",
    "MASKING_CHANCE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26db887c-4d1a-432e-95ac-6438e4cf4e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEncodings(Dataset):\n",
    "    '''\n",
    "    tokenizes, pads, and adds special tokens\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        tokenizer,\n",
    "        source_max_token_len: int,\n",
    "        target_max_token_len: int\n",
    "        ):\n",
    "        self.tokenizer = t5tokenizer\n",
    "        self.data = data\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index:int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        # adds a random mask for answer-agnostic qg\n",
    "        if np.random.rand() > MASKING_CHANCE:\n",
    "            answer = data_row['answer']\n",
    "        else:\n",
    "            answer = MASK_TOKEN\n",
    "            \n",
    "        source_encoding = t5tokenizer(\n",
    "            f\"{answer} {SEP_TOKEN} {data_row['context']}\",\n",
    "            max_length= self.source_max_token_len,\n",
    "            padding='max_length',\n",
    "            truncation= True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "            )\n",
    "    \n",
    "        target_encoding = t5tokenizer(\n",
    "            f\"{data_row['answer']} {SEP_TOKEN} {data_row['question']}\",\n",
    "            max_length=self.target_max_token_len,\n",
    "            padding='max_length',\n",
    "            truncation = True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "            )\n",
    "\n",
    "        labels = target_encoding['input_ids']  \n",
    "        labels[labels == 0] = -100 # masked\n",
    "\n",
    "        encodings = dict(\n",
    "            answer = data_row['answer'],\n",
    "            context = data_row['context'],\n",
    "            question = data_row['question'],\n",
    "            input_ids = source_encoding['input_ids'].flatten(),\n",
    "            attention_mask = source_encoding['attention_mask'].flatten(),\n",
    "            labels=labels.flatten()\n",
    "        )\n",
    "        \n",
    "        return encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694ca11-1a7a-4a87-827d-1a9cb57f5bf5",
   "metadata": {},
   "source": [
    "We will also define a data module to split the data into batches to train our model to prevent out-of-memory errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8190f348-d377-4f10-9a5e-8ff060c67c9f",
   "metadata": {
    "id": "9gAO0iB0LYh0"
   },
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train: pd.DataFrame,\n",
    "        val: pd.DataFrame,\n",
    "        tokenizer,\n",
    "        batch_size,\n",
    "        source_max_token_len: int,\n",
    "        target_max_token_len: int\n",
    "        ): \n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        self.tokenizer = t5tokenizer\n",
    "        self.source_max_token_len = source_max_token_len\n",
    "        self.target_max_token_len = target_max_token_len\n",
    "\n",
    "    def setup(self):\n",
    "        self.train_dataset = DataEncodings(self.train, self.tokenizer, self.source_max_token_len, self.target_max_token_len)\n",
    "        self.val_dataset = DataEncodings(self.val, self.tokenizer, self.source_max_token_len, self.target_max_token_len)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self): \n",
    "        return DataLoader(self.val_dataset, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42a6c50c-b93f-4b73-86f8-de069285d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(train, val, t5tokenizer, batch_size=32, source_max_token_len=128, target_max_token_len=64)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35efdf6c-40ac-4808-bfc4-aeb64ef031fe",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "The [pytorch lightning](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html?highlight=training_step#training-step) library simplifies the model training process. We will be using a T5-small model with an Adafactor optimizer, as [used by the original T5 paper](https://discuss.huggingface.co/t/t5-finetuning-tips/684/3). We will also choose to optimize the [cross-entropy loss](https://discuss.huggingface.co/t/what-is-loss-function-for-t5/11669) for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a6d3ad-5855-4507-b4d5-e0c3554e0a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_epochs = 16\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b289174-219f-405b-a898-5cd402015d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "class T5Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = t5model\n",
    "        self.model.resize_token_embeddings(len(t5tokenizer)) # resizing after adding new tokens to the tokenizer\n",
    "\n",
    "    # feed forward pass\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return output.loss, output.logits\n",
    "\n",
    "    # train model and compute loss\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, output = self(input_ids, attention_mask, labels)\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        return loss\n",
    "\n",
    "    # gets model predictions, returns loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        loss, output = self(input_ids, attention_mask, labels)\n",
    "        self.log('val_loss', loss, prog_bar=True, logger=True, batch_size=batch_size)\n",
    "        return {'val loss': loss}\n",
    "    \n",
    "    # def validation_epoch_end(self, outputs):\n",
    "    #     # outputs = list of dictionaries to print loss\n",
    "    #     avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "    #     tensorboard_logs = {'avg_val_loss': avg_loss}\n",
    "    #     return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adafactor(model.parameters(), scale_parameter=False, relative_step=False, lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04814ca3-f6ac-435d-9e86-f3658810c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5Model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1b049e-966c-4b8c-b5c4-de1725fa85f1",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "As we examine the model architecture, we can choose to freeze/unfreeze layers in the process of optimizing our model, but it is still currently beyond the scope of this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7230255e-762c-4a1e-8b6e-e68618101c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of T5Model(\n",
       "  (model): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32100, 512)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32100, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32100, 512)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 8)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "                (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseReluDense(\n",
       "                (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=32100, bias=False)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "311020ae-6cf2-42fa-9e8a-b313a3d4898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if name in ['Linear']:\n",
    "#         print(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0c3697e-c215-44f4-aa41-08cbd7fa4dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to freeze layers in the vanilla model\n",
    "# for name, param in model.named_parameters():\n",
    "#      if name == \"...\":\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8ec4a7-2b89-4762-ac1f-5faef4701c34",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Pytorch lightning allows us to save checkpoints and picks out the one with the lowest loss function. We can also implement an early stopping function if the loss score keeps rising after 3 consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4249595-cdd0-4569-8ce4-ec3ce8fb0aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "# saving model checkpoints, minimizes val loss\n",
    "callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"t5-chkpt\",\n",
    "    save_top_k=-1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    # fast_dev_run=True,\n",
    "    callbacks=[callback, EarlyStopping(monitor=\"val_loss\")],\n",
    "    max_epochs=num_epochs,\n",
    "    gpus=1,\n",
    "    auto_lr_find=True,\n",
    "    deterministic=True,\n",
    "    log_every_n_steps=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4067f79-321b-4d9a-a5b6-4c6ee43be6f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goeis\\.conda\\envs\\python\\lib\\site-packages\\pytorch_lightning\\core\\datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M\n",
      "-----------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "241.969   Total estimated model params size (MB)\n",
      "C:\\Users\\goeis\\.conda\\envs\\python\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:631: UserWarning: Checkpoint directory C:\\Users\\goeis\\Documents\\GA Stuff\\DSI-working-folder\\QG-System\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goeis\\.conda\\envs\\python\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Global seed set to 25429\n",
      "C:\\Users\\goeis\\.conda\\envs\\python\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9486aa55800a43d1ae7a35f4acebccaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 2737: val_loss reached 1.25404 (best 1.25404), saving model to \"C:\\Users\\goeis\\Documents\\GA Stuff\\DSI-working-folder\\QG-System\\checkpoints\\t5-chkpt-v6.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 5475: val_loss reached 1.23136 (best 1.23136), saving model to \"C:\\Users\\goeis\\Documents\\GA Stuff\\DSI-working-folder\\QG-System\\checkpoints\\t5-chkpt-v7.ckpt\" as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 8213: val_loss reached 1.23399 (best 1.23136), saving model to \"C:\\Users\\goeis\\Documents\\GA Stuff\\DSI-working-folder\\QG-System\\checkpoints\\t5-chkpt-v8.ckpt\" as top 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 10951: val_loss reached 1.24462 (best 1.23136), saving model to \"C:\\Users\\goeis\\Documents\\GA Stuff\\DSI-working-folder\\QG-System\\checkpoints\\t5-chkpt-v9.ckpt\" as top 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 13689: val_loss reached 1.24307 (best 1.23136), saving model to \"C:\\Users\\goeis\\Documents\\GA Stuff\\DSI-working-folder\\QG-System\\checkpoints\\t5-chkpt-v10.ckpt\" as top 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = T5Model()\n",
    "    trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a190fb8-c20c-43c3-848e-33ddf4446b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2314, device='cuda:0')\n",
      "C:\\Users\\goeis\\Documents\\GA Stuff\\DSI-working-folder\\QG-System\\checkpoints\\t5-chkpt-v7.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(callback.best_model_score)\n",
    "print(callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794d41e-845b-4d30-8e0c-46c6009d64e4",
   "metadata": {},
   "source": [
    "# Model Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f720c9-dafc-4cea-9222-f23d6c917109",
   "metadata": {},
   "source": [
    "We use the tensorboard integration with pytorch to view how our model was trained and observe how the loss function changed over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aef58932-5033-49fe-a8ad-15bf12165ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44543e01-cf27-4e96-b6db-3df7bb2e6733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3456), started 4 days, 3:38:24 ago. (Use '!kill 3456' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5c9f8c462b1ec277\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5c9f8c462b1ec277\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir lightning_logs/ --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da176ece-3985-49ec-86fd-7d2f5559879f",
   "metadata": {},
   "source": [
    "# Viewing Results\n",
    "\n",
    "After training our model, it is time to view the results. In the process of decoding the model, there are a few important hyperparameters we can tune:\n",
    "- beam search: considers the words around the predicted text when predicting the next word. computationally expensive but gives better results\n",
    "- repetition/length penalty: adds weights that penalizes answers being too long or having repeated words\n",
    "- [temperature](https://github.com/huggingface/transformers/issues/2029): \"controls the randomness of predictions by scaling the logits before applying softmax\", which controls the \"conservativeness\" of the model in prediction\n",
    "\n",
    "As a general principle, we tried to optimize the hyperparameters so that there would be greater generalizability of the model - such that more correct question-answer pairs can be generated for unseen data. We will then be scoring our models using the BLEU score from `nltk` and the cosine similarity score from `spaCy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40b2e449-b731-4f49-9065-375280c90d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_model_dir = 'checkpoints/t5-chkpt-v2.ckpt'\n",
    "# best_model = model.load_from_checkpoint(best_model_dir)\n",
    "\n",
    "best_model = model.load_from_checkpoint(callback.best_model_path)\n",
    "best_model.freeze()\n",
    "# best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "576c3f4d-f256-405c-a482-586aa2fc3e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model: T5Model, answer:str, context:str) -> str:\n",
    "    source_encoding = t5tokenizer(\n",
    "        f\"{answer} {SEP_TOKEN} {context}\",\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    generated_ids=model.model.generate(\n",
    "        input_ids=source_encoding['input_ids'],\n",
    "        attention_mask=source_encoding['attention_mask'],\n",
    "        num_beams=20,\n",
    "        max_length=126,\n",
    "        repetition_penalty=2.5,\n",
    "        length_penalty=0.8,\n",
    "        temperature=0.6,\n",
    "        early_stopping=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "\n",
    "    preds = {\n",
    "        t5tokenizer.decode(generated_id, skip_special_tokens=False, clean_up_tokenization_spaces=True)\n",
    "        for generated_id in generated_ids\n",
    "    }\n",
    "\n",
    "    return ''.join(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1844bff-db6e-430a-9aa4-9baa999c72a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is using GPU!\n"
     ]
    }
   ],
   "source": [
    "is_using_gpu = spacy.require_gpu()\n",
    "if is_using_gpu:\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "    print('spaCy is using GPU!')\n",
    "else:\n",
    "    print('spaCy is not using GPU...')\n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df573697-18d6-4b6e-b517-b0ccc843fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printBold(string):\n",
    "    display(Markdown('**' + string + '**'))\n",
    "\n",
    "\n",
    "def show_result(generated:str, answer:str, context:str, original_question:str=''):\n",
    "\n",
    "    regex = r\"(?<=>)(.*?)(?=<)\"\n",
    "    matches = re.findall(regex, generated)\n",
    "    matches[1] = matches[1][5:]\n",
    "    final = {cat: match.strip() for cat, match in zip(['Answer', 'Question'], matches)}\n",
    "    \n",
    "    printBold('Context')\n",
    "    print(context)\n",
    "    printBold('Answer')\n",
    "    print(answer)\n",
    "    printBold('Generated Answer/Question')\n",
    "    print(final)\n",
    "    if original_question:\n",
    "        printBold('Original Question')\n",
    "        print(original_question)\n",
    "        gen = nlp(matches[1])\n",
    "        ori = nlp(original_question)\n",
    "        bleu_score = sentence_bleu(matches[1], original_question, smoothing_function=SmoothingFunction().method5)\n",
    "        cs_score = ori.similarity(gen)\n",
    "        printBold('Scores')\n",
    "        print(f\"BLEU: {bleu_score}\")\n",
    "        print(f'Cosine Similarity: {cs_score}')\n",
    "        return bleu_score, cs_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0774b48a-6f68-4a4a-8409-0672d40e6a96",
   "metadata": {},
   "source": [
    "## Validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6044d54-4545-4e0d-99d0-7766e8ec8ef2",
   "metadata": {},
   "source": [
    "We will start with 5 sample questions from the validation set to score the model predictions. We will then provide 5 passages from different contexts to see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e13aadfe-1fe7-40e7-aebf-cee686e5367d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d7d2f-d4ab-4526-944e-8984d2ca90e4",
   "metadata": {},
   "source": [
    "### Unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ae0e3f09-89e1-4d12-8a0f-2eb3af381072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question 1**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With International Criminal Court trial dates in 2013 for both President Kenyatta and Deputy President William Ruto related to the 2007 election aftermath, US President Barack Obama chose not to visit the country during his mid-2013 African trip. Later in the summer, Kenyatta visited China at the invitation of President Xi Jinping after a stop in Russia and not having visited the United States as president. In July 2015 Obama visited Kenya, as the first American president to visit the country while in office.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US President Barack Obama\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'US President Barack Obama', 'Question': 'Who chose not to visit Kenya during his 2013 African trip?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Original Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who decided not to come visit the country in 2013?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Scores**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.11547005383792518\n",
      "Cosine Similarity: 0.9212935566902161\n",
      "**************************************************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question 2**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the first known experiments on the relationship between combustion and air was conducted by the 2nd century BCE Greek writer on mechanics, Philo of Byzantium. In his work Pneumatica, Philo observed that inverting a vessel over a burning candle and surrounding the vessel's neck with water resulted in some water rising into the neck. Philo incorrectly surmised that parts of the air in the vessel were converted into the classical element fire and thus were able to escape through pores in the glass. Many centuries later Leonardo da Vinci built on Philo's work by observing that a portion of air is consumed during combustion and respiration.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fire\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'fire', 'Question': 'What did Philo believe parts of the vessel were converted into?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Original Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did Philo incorrectly assume that the air became?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Scores**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.10454078948289519\n",
      "Cosine Similarity: 0.9039292335510254\n",
      "**************************************************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question 3**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the mid-Eocene, it is believed that the drainage basin of the Amazon was split along the middle of the continent by the Purus Arch. Water on the eastern side flowed toward the Atlantic, while to the west water flowed toward the Pacific across the Amazonas Basin. As the Andes Mountains rose, however, a large basin was created that enclosed a lake; now known as the Solimões Basin. Within the last 5–10 million years, this accumulating water broke through the Purus Arch, joining the easterly flow toward the Atlantic.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the mid-Eocene, it is believed that the drainage basin of the Amazon was split along the middle of the continent by the Purus Arch.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'During the mid-Eocene, it is believed that the drainage basin of the Amazon was split along the middle of the continent by the Purus Arch.', 'Question': 'What happened to the drainage basin?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Original Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In which point did the drainage basin of the Amazon split?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Scores**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.09733107986338517\n",
      "Cosine Similarity: 0.9272109866142273\n",
      "**************************************************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question 4**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pope Leo X was used to reformers and heretics, and he responded slowly, \"with great care as is proper.\" Over the next three years he deployed a series of papal theologians and envoys against Luther, which served only to harden the reformer's anti-papal theology. First, the Dominican theologian Sylvester Mazzolini drafted a heresy case against Luther, whom Leo then summoned to Rome. The Elector Frederick persuaded the pope to have Luther examined at Augsburg, where the Imperial Diet was held. There, in October 1518, under questioning by papal legate Cardinal Cajetan Luther stated that he did not consider the papacy part of the biblical Church because historistical interpretation of Bible prophecy concluded that the papacy was the Antichrist. The prophecies concerning the Antichrist soon became the center of controversy. The hearings degenerated into a shouting match. More than his writing the 95 Theses, Luther's confrontation with the church cast him as an enemy of the pope. Cajetan's original instructions had been to arrest Luther if he failed to recant, but the legate desisted from doing so. Luther slipped out of the city at night, unbeknownst to Cajetan.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrest\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'arrest', 'Question': \"What did Cajetan's original instructions consist of?\"}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Original Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What were the papal legate's orders from the Pope?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Scores**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.10264004785593345\n",
      "Cosine Similarity: 0.7661088109016418\n",
      "**************************************************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question 5**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smaller galleries cover Korea, the Himalayan kingdoms and South East Asia. Korean displays include green-glazed ceramics, silk embroideries from officials' robes and gleaming boxes inlaid with mother-of-pearl made between 500 AD and 2000. Himalayan items include important early Nepalese bronze sculptures, repoussé work and embroidery. Tibetan art from the 14th to the 19th century is represented by notable 14th- and 15th-century religious images in wood and bronze, scroll paintings and ritual objects. Art from Thailand, Burma, Cambodia, Indonesia and Sri Lanka in gold, silver, bronze, stone, terracotta and ivory represents these rich and complex cultures, the displays span the 6th to 19th centuries. Refined Hindu and Buddhist sculptures reflect the influence of India; items on show include betel-nut cutters, ivory combs and bronze palanquin hooks.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sri Lanka\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'Sri Lanka', 'Question': 'In gold, silver, bronze, stone, terracotta and ivory, what other country has art from?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Original Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which South Asian island nation is represented in the V&A collection?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Scores**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.09204134726211426\n",
      "Cosine Similarity: 0.7634811401367188\n",
      "**************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "avg_bleu, avg_cs = [], []\n",
    "np.random.seed(12)\n",
    "for i in range(5):\n",
    "    printBold(f\"Question {i+1}\")\n",
    "    sample_question = val.iloc[np.random.randint(len(val)-1)]\n",
    "    generated = generate(best_model, sample_question['answer'], sample_question['context'])\n",
    "    blue, cs = show_result(generated, sample_question['answer'], sample_question['context'], sample_question['question'])\n",
    "    avg_bleu.append(blue) # to avoid clashes\n",
    "    avg_cs.append(cs)\n",
    "    print('*'*89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bd05b97-e2f4-4e7c-b401-a6a4346dc216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Average BLEU Score**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10240466366045065\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Average Cosine Similarity Score**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8564047455787659\n"
     ]
    }
   ],
   "source": [
    "printBold('Average BLEU Score')\n",
    "print(f\"{sum(avg_bleu)/len(avg_bleu)}\")\n",
    "printBold('Average Cosine Similarity Score')\n",
    "print(f\"{sum(avg_cs)/len(avg_cs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7363283-5557-4b09-bd61-d14f274b37cc",
   "metadata": {},
   "source": [
    "### Masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c220aadd-0bad-49dc-afcd-f0e6282d6328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Question 1**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With International Criminal Court trial dates in 2013 for both President Kenyatta and Deputy President William Ruto related to the 2007 election aftermath, US President Barack Obama chose not to visit the country during his mid-2013 African trip. Later in the summer, Kenyatta visited China at the invitation of President Xi Jinping after a stop in Russia and not having visited the United States as president. In July 2015 Obama visited Kenya, as the first American president to visit the country while in office.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'President Xi Jinping', 'Question': 'Who was the first American president to visit Kenya?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Original Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who decided not to come visit the country in 2013?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Scores**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.1052060490523318\n",
      "Cosine Similarity: 0.8921566009521484\n",
      "**************************************************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question 2**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the first known experiments on the relationship between combustion and air was conducted by the 2nd century BCE Greek writer on mechanics, Philo of Byzantium. In his work Pneumatica, Philo observed that inverting a vessel over a burning candle and surrounding the vessel's neck with water resulted in some water rising into the neck. Philo incorrectly surmised that parts of the air in the vessel were converted into the classical element fire and thus were able to escape through pores in the glass. Many centuries later Leonardo da Vinci built on Philo's work by observing that a portion of air is consumed during combustion and respiration.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'Philo of Byzantium', 'Question': 'Who conducted one of the first known experiments on the relationship between combustion and air?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Original Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What did Philo incorrectly assume that the air became?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Scores**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.10691671651659736\n",
      "Cosine Similarity: 0.824653148651123\n",
      "**************************************************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question 3**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During the mid-Eocene, it is believed that the drainage basin of the Amazon was split along the middle of the continent by the Purus Arch. Water on the eastern side flowed toward the Atlantic, while to the west water flowed toward the Pacific across the Amazonas Basin. As the Andes Mountains rose, however, a large basin was created that enclosed a lake; now known as the Solimões Basin. Within the last 5–10 million years, this accumulating water broke through the Purus Arch, joining the easterly flow toward the Atlantic.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'mid-Eocene', 'Question': 'When was the drainage basin of the Amazon split?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Original Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In which point did the drainage basin of the Amazon split?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Scores**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.11060349984475588\n",
      "Cosine Similarity: 0.9798660278320312\n",
      "**************************************************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question 4**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pope Leo X was used to reformers and heretics, and he responded slowly, \"with great care as is proper.\" Over the next three years he deployed a series of papal theologians and envoys against Luther, which served only to harden the reformer's anti-papal theology. First, the Dominican theologian Sylvester Mazzolini drafted a heresy case against Luther, whom Leo then summoned to Rome. The Elector Frederick persuaded the pope to have Luther examined at Augsburg, where the Imperial Diet was held. There, in October 1518, under questioning by papal legate Cardinal Cajetan Luther stated that he did not consider the papacy part of the biblical Church because historistical interpretation of Bible prophecy concluded that the papacy was the Antichrist. The prophecies concerning the Antichrist soon became the center of controversy. The hearings degenerated into a shouting match. More than his writing the 95 Theses, Luther's confrontation with the church cast him as an enemy of the pope. Cajetan's original instructions had been to arrest Luther if he failed to recant, but the legate desisted from doing so. Luther slipped out of the city at night, unbeknownst to Cajetan.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'Pope Leo X', 'Question': 'Who was used to reformers and heretics?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Original Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What were the papal legate's orders from the Pope?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Scores**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.10007404665953515\n",
      "Cosine Similarity: 0.744892418384552\n",
      "**************************************************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Question 5**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smaller galleries cover Korea, the Himalayan kingdoms and South East Asia. Korean displays include green-glazed ceramics, silk embroideries from officials' robes and gleaming boxes inlaid with mother-of-pearl made between 500 AD and 2000. Himalayan items include important early Nepalese bronze sculptures, repoussé work and embroidery. Tibetan art from the 14th to the 19th century is represented by notable 14th- and 15th-century religious images in wood and bronze, scroll paintings and ritual objects. Art from Thailand, Burma, Cambodia, Indonesia and Sri Lanka in gold, silver, bronze, stone, terracotta and ivory represents these rich and complex cultures, the displays span the 6th to 19th centuries. Refined Hindu and Buddhist sculptures reflect the influence of India; items on show include betel-nut cutters, ivory combs and bronze palanquin hooks.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'gleaming boxes', 'Question': 'What was made between 500 AD and 2000?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Original Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which South Asian island nation is represented in the V&A collection?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Scores**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.08460366263487269\n",
      "Cosine Similarity: 0.7626774907112122\n",
      "**************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "avg_bleu, avg_cs = [], []\n",
    "np.random.seed(12)\n",
    "for i in range(5):\n",
    "    printBold(f\"Question {i+1}\")\n",
    "    sample_question = val.iloc[np.random.randint(len(val)-1)]\n",
    "    generated = generate(best_model, '[MASK]', sample_question['context'])\n",
    "    blue, cs = show_result(generated, '[MASK]', sample_question['context'], sample_question['question'])\n",
    "    avg_bleu.append(blue) # to avoid clashes\n",
    "    avg_cs.append(cs)\n",
    "    print('**************************************************************************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57bf3c94-5f5f-413b-a653-406424414d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Average BLEU Score**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10148079494161859\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Average Cosine Similarity Score**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8408491373062134\n"
     ]
    }
   ],
   "source": [
    "printBold('Average BLEU Score')\n",
    "print(f\"{sum(avg_bleu)/len(avg_bleu)}\")\n",
    "printBold('Average Cosine Similarity Score')\n",
    "print(f\"{sum(avg_cs)/len(avg_cs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af3147-3e24-4ae2-8f2d-b1f339e7202f",
   "metadata": {},
   "source": [
    "## Testing on new data\n",
    "\n",
    "### Unmasked\n",
    "\n",
    "5 passages:\n",
    "1. simple passage\n",
    "2. [bible ESV](https://www.biblegateway.com/passage/?search=Galatians+5&version=ESV)\n",
    "3. [jimi hendrix wiki](https://en.wikipedia.org/wiki/jimi_hendrix)\n",
    "4. [de Broglie's nobel lecture](https://www.nobelprize.org/uploads/2016/04/broglie-lecture.pdf) (convert to txt, remove equations)\n",
    "5. [Zhuangzi](https://scholarworks.iu.edu/dspace/bitstream/handle/2022/23427/Zhuangzi.pdf?sequence=2&isAllowed=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "799c059f-9cf1-4dc7-9602-f8fd282c1275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stale\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'stale', 'Question': 'The issue has been automatically marked as what?'}\n"
     ]
    }
   ],
   "source": [
    "context = 'This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.'\n",
    "answer = 'stale'\n",
    "answer_2 = '[MASK]'\n",
    "\n",
    "generated = generate(best_model, answer, context)\n",
    "show_result(generated, answer, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8dcde391-6952-41a2-af19-c3e9376f0294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For you were called to freedom, brothers. Only do not use your freedom as an opportunity for the flesh, but through love serve one another. For the whole law is fulfilled in one word: \"You shall love your neighbor as yourself.\" But if you bite and devour one another, watch out that you are not consumed by one another.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you shall love your neighbor as yourself\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'you shall love your neighbor as yourself', 'Question': 'What is the whole law fulfilled in one word?'}\n"
     ]
    }
   ],
   "source": [
    "gal = 'For you were called to freedom, brothers. Only do not use your freedom as an opportunity for the flesh, but through love serve one another. For the whole law is fulfilled in one word: \"You shall love your neighbor as yourself.\" But if you bite and devour one another, watch out that you are not consumed by one another.'\n",
    "gal_ans = 'you shall love your neighbor as yourself'\n",
    "gal_ans_2 = '[MASK]'\n",
    "\n",
    "generated = generate(best_model, gal_ans, gal)\n",
    "show_result(generated, gal_ans, gal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f31bb385-73a6-45f6-b648-2eff81b2dd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Born in Seattle, Washington, Hendrix began playing guitar at the age of 15. In 1961, he enlisted in the US Army, but was discharged the following year. Soon afterward, he moved to Clarksville then Nashville, Tennessee, and began playing gigs on the chitlin' circuit, earning a place in the Isley Brothers' backing band and later with Little Richard, with whom he continued to work through mid-1965. He then played with Curtis Knight and the Squires before moving to England in late 1966 after bassist Chas Chandler of the Animals became his manager. Within months, Hendrix had earned three UK top ten hits with the Jimi Hendrix Experience: 'Hey Joe', 'Purple Haze', and 'The Wind Cries Mary'. He achieved fame in the US after his performance at the Monterey Pop Festival in 1967, and in 1968 his third and final studio album, Electric Ladyland, reached number one in the US. The double LP was Hendrix's most commercially successful release and his first and only number one album. The world's highest-paid performer, he headlined the Woodstock Festival in 1969 and the Isle of Wight Festival in 1970 before his accidental death in London from barbiturate-related asphyxia on September 18, 1970.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hey Joe', 'Purple Haze', and 'The Wind Cries Mary'\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': \"'Hey Joe', 'Purple Haze', and 'The Wind Cries Mary'\", 'Question': 'What three hits did Hendrix have in the UK?'}\n"
     ]
    }
   ],
   "source": [
    "jimi = \"Born in Seattle, Washington, Hendrix began playing guitar at the age of 15. In 1961, he enlisted in the US Army, but was discharged the following year. Soon afterward, he moved to Clarksville then Nashville, Tennessee, and began playing gigs on the chitlin' circuit, earning a place in the Isley Brothers' backing band and later with Little Richard, with whom he continued to work through mid-1965. He then played with Curtis Knight and the Squires before moving to England in late 1966 after bassist Chas Chandler of the Animals became his manager. Within months, Hendrix had earned three UK top ten hits with the Jimi Hendrix Experience: 'Hey Joe', 'Purple Haze', and 'The Wind Cries Mary'. He achieved fame in the US after his performance at the Monterey Pop Festival in 1967, and in 1968 his third and final studio album, Electric Ladyland, reached number one in the US. The double LP was Hendrix's most commercially successful release and his first and only number one album. The world's highest-paid performer, he headlined the Woodstock Festival in 1969 and the Isle of Wight Festival in 1970 before his accidental death in London from barbiturate-related asphyxia on September 18, 1970.\"\n",
    "jimi_ans = \"'Hey Joe', 'Purple Haze', and 'The Wind Cries Mary'\"\n",
    "jimi_ans_2 = '[MASK]'\n",
    "\n",
    "\n",
    "generated = generate(best_model, jimi_ans, jimi)\n",
    "show_result(generated, jimi_ans, jimi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ce9e2b2a-8dc1-40ac-9174-dd0edbbc2f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The existence of a granular structure of light and of other radiations was confirmed by the discovery of the photoelectric effect. If a beam of light or of X-rays falls on a piece of matter, the latter will emit rapidly moving electrons. The kinetic energy of these electrons increases linearly with the frequency of the incident radiation and is independent of its intensity. This phenomenon can be explained simply by assuming that the radiation is composed of quanta hv capable of yielding all their energy to an electron of the irradiated body: one is thus led to the theory of light quanta proposed by Einstein in 1905 and which is, after all, a reversion to Newton’s corpuscular theory, completed by the relation for the proportionality between the energy of the corpuscles and the frequency.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "granular structure of light\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'granular structure of light', 'Question': 'What was confirmed by the photoelectric effect?'}\n"
     ]
    }
   ],
   "source": [
    "debroglie = 'The existence of a granular structure of light and of other radiations was confirmed by the discovery of the photoelectric effect. If a beam of light or of X-rays falls on a piece of matter, the latter will emit rapidly moving electrons. The kinetic energy of these electrons increases linearly with the frequency of the incident radiation and is independent of its intensity. This phenomenon can be explained simply by assuming that the radiation is composed of quanta hv capable of yielding all their energy to an electron of the irradiated body: one is thus led to the theory of light quanta proposed by Einstein in 1905 and which is, after all, a reversion to Newton’s corpuscular theory, completed by the relation for the proportionality between the energy of the corpuscles and the frequency.'\n",
    "db_ans = 'granular structure of light'\n",
    "db_ans_2 = '[MASK]'\n",
    "\n",
    "generated = generate(best_model, db_ans, debroglie)\n",
    "show_result(generated, db_ans, debroglie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "858607dc-0d03-4d0b-9feb-408640e56db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The god of the Southern Sea was Swift; the god of the Northern Sea was Sudden. The god of the center was Hundun. Swift and Sudden would often meet in the land of Hundun, and Hundun would host them with great courtesy. Swift and Sudden made a plan to return Hundun’s generosity. “All men have seven orifices,” they said, “so that they can see and hear, eat and breathe. Hundun alone has none. Why don’t we bore these for him?” Each day, they bored one orifice, and on the seventh day, Hundun died.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'none', 'Question': 'How many orifices did Hundun have?'}\n"
     ]
    }
   ],
   "source": [
    "zhuangzi = 'The god of the Southern Sea was Swift; the god of the Northern Sea was Sudden. The god of the center was Hundun. Swift and Sudden would often meet in the land of Hundun, and Hundun would host them with great courtesy. Swift and Sudden made a plan to return Hundun’s generosity. “All men have seven orifices,” they said, “so that they can see and hear, eat and breathe. Hundun alone has none. Why don’t we bore these for him?” Each day, they bored one orifice, and on the seventh day, Hundun died.'\n",
    "zhuangzi_ans = 'none'\n",
    "zhuangzi_ans_2 = '[MASK]'\n",
    "\n",
    "generated = generate(best_model, zhuangzi_ans, zhuangzi)\n",
    "show_result(generated, zhuangzi_ans, zhuangzi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea0cb7-3fa0-4f71-9399-a4ddfd99b592",
   "metadata": {},
   "source": [
    "### Masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "86a6e9e0-58af-4a69-8f64-679a62893e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'it has not had recent activity', 'Question': 'Why has this issue been marked as stale?'}\n"
     ]
    }
   ],
   "source": [
    "generated = generate(best_model, answer_2, context)\n",
    "show_result(generated, answer_2, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1cf4d105-6fc5-4376-929b-0f20ff580185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For you were called to freedom, brothers. Only do not use your freedom as an opportunity for the flesh, but through love serve one another. For the whole law is fulfilled in one word: \"You shall love your neighbor as yourself.\" But if you bite and devour one another, watch out that you are not consumed by one another.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'if you bite and devour one another, watch out that you are not consumed by one another', 'Question': 'What happens when you bite and devour one another?'}\n"
     ]
    }
   ],
   "source": [
    "generated = generate(best_model, gal_ans_2, gal)\n",
    "show_result(generated, gal_ans_2, gal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0997a1df-1c82-4295-8238-8a86ef57efad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Born in Seattle, Washington, Hendrix began playing guitar at the age of 15. In 1961, he enlisted in the US Army, but was discharged the following year. Soon afterward, he moved to Clarksville then Nashville, Tennessee, and began playing gigs on the chitlin' circuit, earning a place in the Isley Brothers' backing band and later with Little Richard, with whom he continued to work through mid-1965. He then played with Curtis Knight and the Squires before moving to England in late 1966 after bassist Chas Chandler of the Animals became his manager. Within months, Hendrix had earned three UK top ten hits with the Jimi Hendrix Experience: 'Hey Joe', 'Purple Haze', and 'The Wind Cries Mary'. He achieved fame in the US after his performance at the Monterey Pop Festival in 1967, and in 1968 his third and final studio album, Electric Ladyland, reached number one in the US. The double LP was Hendrix's most commercially successful release and his first and only number one album. The world's highest-paid performer, he headlined the Woodstock Festival in 1969 and the Isle of Wight Festival in 1970 before his accidental death in London from barbiturate-related asphyxia on September 18, 1970.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': '1969', 'Question': 'In what year did Hendrix headline the Isle of Wight Festival?'}\n"
     ]
    }
   ],
   "source": [
    "generated = generate(best_model, jimi_ans_2, jimi)\n",
    "show_result(generated, jimi_ans_2, jimi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a27bb45f-1f6f-4655-adc4-742f86cbdc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The existence of a granular structure of light and of other radiations was confirmed by the discovery of the photoelectric effect. If a beam of light or of X-rays falls on a piece of matter, the latter will emit rapidly moving electrons. The kinetic energy of these electrons increases linearly with the frequency of the incident radiation and is independent of its intensity. This phenomenon can be explained simply by assuming that the radiation is composed of quanta hv capable of yielding all their energy to an electron of the irradiated body: one is thus led to the theory of light quanta proposed by Einstein in 1905 and which is, after all, a reversion to Newton’s corpuscular theory, completed by the relation for the proportionality between the energy of the corpuscles and the frequency.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'quantum hv capable of yielding all their energy to an electron of the irradiated body', 'Question': 'What is the theory of light quanta?'}\n"
     ]
    }
   ],
   "source": [
    "generated = generate(best_model, db_ans_2, debroglie)\n",
    "show_result(generated, db_ans_2, debroglie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6f0d8c1c-a6bb-414e-8562-51a1bb866d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Context**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The god of the Southern Sea was Swift; the god of the Northern Sea was Sudden. The god of the center was Hundun. Swift and Sudden would often meet in the land of Hundun, and Hundun would host them with great courtesy. Swift and Sudden made a plan to return Hundun’s generosity. “All men have seven orifices,” they said, “so that they can see and hear, eat and breathe. Hundun alone has none. Why don’t we bore these for him?” Each day, they bored one orifice, and on the seventh day, Hundun died.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Answer**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MASK]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Answer/Question**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer': 'Hundun', 'Question': 'Who was the god of the Northern Sea?'}\n"
     ]
    }
   ],
   "source": [
    "generated = generate(best_model, zhuangzi_ans_2, zhuangzi)\n",
    "show_result(generated, zhuangzi_ans_2, zhuangzi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
